# k8s

- [강의 사이트](https://kubetm.github.io/k8s/)

- [k8s](#k8s)
  - [기초](#기초)
    - [CLI 환경 필수 명령어](#cli-환경-필수-명령어)
  - [Pod](#pod)
    - [Container](#container)
    - [Label](#label)
    - [Node Schedule](#node-schedule)
  - [Service](#service)
    - [ClusterIP](#clusterip)
    - [NodePort](#nodeport)
    - [Load Balancer](#load-balancer)
  - [Volume](#volume)
    - [emptyDir](#emptydir)
    - [hostPath](#hostpath)
    - [PVC / PV](#pvc--pv)
  - [ConfigMap](#configmap)
    - [Env - Literal](#env---literal)
    - [Env - File](#env---file)
    - [Volume Mount - File](#volume-mount---file)
  - [Namespace](#namespace)
    - [Namespace Exception](#namespace-exception)
    - [ResourceQuota](#resourcequota)
    - [LimitRange](#limitrange)
    - [LimitRange Exception](#limitrange-exception)
  - [Controller](#controller)
    - [Auto Healing](#auto-healing)
    - [Auto Scaling](#auto-scaling)
    - [Software Update](#software-update)
    - [Job_](#job_)
  - [Replication](#replication)
    - [Template, Replicas](#template-replicas)
    - [Updating Controller](#updating-controller)
    - [Selector](#selector)
  - [Deployment](#deployment)
    - [ReCreate](#recreate)
    - [RollingUpdate](#rollingupdate)
    - [Blue/Green](#bluegreen)
    - [Canary](#canary)
  - [DaemonSet, Job, CronJob](#daemonset-job-cronjob)
    - [DaemonSet - Hostport](#daemonset---hostport)
    - [DaemonSet - NodeSelector](#daemonset---nodeselector)
    - [Job](#job)
    - [CronJob](#cronjob)
    - [Cronjob - ConcurrencyPolicy](#cronjob---concurrencypolicy)
    - [CronJob - 1.19ver 이후 변경사항](#cronjob---119ver-이후-변경사항)

---

## 기초

- 각 노드 명, 파드 명, 서비스 명 등은 알아서 맞춰서 한다.

### CLI 환경 필수 명령어

1. kubectl exec 파드명 -it /bin/bash
2. kubectl exec 파드명 -c 컨테이너명 -it /bin/bash

## Pod

- 하나의 컨테이너가 여러 개의 포트를 가질 수 있다.
- 컨테이너끼리 해당 포트를 통하여 통신이 가능하다.
- pod는 생성 할 때마다 IP를 자동으로 할당 받는다.
- 파드에 오류가 생길 시에 재시작한다. 이때 IP는 변경된다.

![pod](https://kubetm.github.io/img/practice/beginner/Pod%20with%20Container%2C%20Label%2C%20Node%20Schedule%20for%20Kubernetes.jpg)

### Container

![container](https://kubetm.github.io/img/practice/beginner/Pod%20with%20Container%20Port%20for%20Kubernetes.jpg)

1. Pod

    ```yaml
    apiVersion: v1
    kind: Pod                   # Pod, Service, ReplicationController 구분 여부
    metadata:
      name: pod-1               # 해당 Pod명 선언
    spec:
      containers:
      - name: container1        # Pod안의 container명 선언
        image: kubetm/p8000     # container에 올릴 이미지 선언
        ports:
        - containerPort: 8000   # 컨테이너가 바깥으로 연결되는 포트 선언
      - name: container2
        image: kubetm/p8080
        ports:
        - containerPort: 8080
    ```

2. ReplicationController - 추후 추가예정

- pod를 만들어주고 종료될 때 자동으로 다시 실행시켜주는 관리자 역할

    ```yaml
    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: replication-1
    spec:
      replicas: 1
      selector:
        app: rc
      template:
        metadata:
          name: pod-1
          labels:
            app: rc
        spec:
          containers:
          - name: container
            image: kubetm/init
    ```

---

### Label

- key:value 한 쌍
- 한 pod에 여러 개의 라벨을 달 수 있다.
- 원하는 파드를 필터링하기 위함

![label](https://kubetm.github.io/img/practice/beginner/Pod%20with%20Label%20Selector%20for%20Kubernetes.jpg)

1. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-2
      labels:
        type: web           # label 선언 key:value의 형식으로 작성된다.
        lo: dev
    spec:
      containers:
      - name: container
        image: kubetm/init
    ```

2. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-1
    spec:
      selector:
        type: web           # 원하는 label을 key:value 형식으로 맞추면 원하는 pod만 연결 가능
      ports:
      - port: 8080
    ```

---

### Node Schedule

- pod가 직접 노드를 선택할 수 있게 할 수 있다.
- 리미트 설정을 안할 시 파드가 지속하여 노드의 자원을 소모하여 다른 파드와 같이 종료될 수 있다.

![node schedule](https://kubetm.github.io/img/practice/beginner/Pod%20with%20Node%20Schedule%20for%20Kubernetes.jpg)

1. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-3
    spec:
      nodeSelector:
        kubernetes.io/hostname: k8s-node1   # Pod를 원하는 work node에 배치할 수 있다.
      containers:
      - name: container
        image: kubetm/init
    ```

2. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-4
    spec:
      containers:
      - name: container
        image: kubetm/init
        resources:
          requests:
            memory: 2Gi     # 권장사항 - 이에 맞춰서 적합한 노드에 배치된다.
          limits:
            memory: 3Gi     # 최대사항 - 이를 넘으면 memory는 파드를 종료, cpu는 해당 사항까지 맞춘다.
    ```

---

## Service

- pod는 언제든 문제가 생겨 종료되어 다시 실행되면 ip가 바뀌기 때문에 신뢰성이 떨어진다.
- service는 사용자가 지우지 않으면 삭제되거나 재생성되지 않기 때문에 언제든 pod에 접근 가능하다.

![service](https://kubetm.github.io/img/practice/beginner/Service%20with%20ClusterIP%2C%20NodePort%2C%20LoadBalancer.jpg)

### ClusterIP

- k8s cluster내에서만 접근가능하다.
- 외부 접근 불가능
- 여러 개의 파드를 연결 가능하다.
- 외부에서 접근 x
- 운영자 | 대시보드 관리, 각 파드에 서비스 상태를 디버깅

![cluster ip](https://kubetm.github.io/img/practice/beginner/Service%20with%20ClusterIP%20for%20Kubernetes.jpg)

1. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-1
      labels:
         app: pod
    spec:
      nodeSelector:
        kubernetes.io/hostname: k8s-node1
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
    ```

2. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-1
    spec:
      selector:
        app: pod
      ports:
      - port: 9000          # cluster ip가 공개되는 port
        targetPort: 8080    # pod에 연결되는 port
    ```

---

### NodePort

- Cluster ip의 특징을 포함한다.
- 모든 Node에 똑같은 Port를 할당해서 해당IP, Port를 이용하여 접속이 가능하다.
- 물리적인 호스트에 ip를 통하여 파드에 접근 / 내부망 안에서 접근해야할 때 / 일시적인 외부 연결할 때 사용한다.

![nodeport](https://kubetm.github.io/img/practice/beginner/Service%20with%20NodePort%20for%20Kubernetes.jpg)

1. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-2
    spec:
      selector:
        app: pod
      ports:
      - port: 9000                  # cluster IP Port
        targetPort: 8080
        nodePort: 30000             # NodePort - 미 작성시 랜덤하게 배정
      type: NodePort                # cluster, node, load balance인지 여부 기본값은 cluster
      externalTrafficPolicy: Local  # 이 옵션을 설정하면 배정된 노드에서만 연결됨
    ```

---

### Load Balancer

- nodeport의 특징을 포함한다.
- 플러그 인을 설치하여 각 노드를 외부에서 접근 가능하게한다.
- 외부에 서비스를 노출하기 위하여 사용한다.

![load balancer](https://kubetm.github.io/img/practice/beginner/Service%20with%20LoadBalancer%20for%20Kubernetes.jpg)

1. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-3
    spec:
      selector:
        app: pod
      ports:
      - port: 9000
        targetPort: 8080
      type: LoadBalancer            # 외부에 공개하려면 따로 플러그인 설치가 필요하다.
    ```

---

## Volume

- 컨테이너끼리 데이터를 공유하기 위하여 볼륨을 사용한다.
- 최초 볼륨 생성 시 항상 비어있다.
- F/E, B/E로 나누어진 컨테이너가 있을 때 데이터가 들어오면 따로 주고 받을 필요가 없이 마운트 된 볼륨에서 사용가능하다.

![volume](https://kubetm.github.io/img/practice/beginner/Volume%20with%20emptyDir%2C%20hostPath%2C%20PV%2C%20PVC%2C%20for%20Kubernetes.jpg)

### emptyDir

- 볼륨은 파드안에서 생성된다.
- 오류가 발생되어 파드가 재생성되면 안의 볼륨도 전부 삭제된다.

![emptyDir](https://kubetm.github.io/img/practice/beginner/Volume%20with%20emptyDir%20for%20Kubernetes.jpg)

1. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-volume-1
    spec:
      containers:
      - name: container1
        image: kubetm/init
        volumeMounts:
        - name: empty-dir       # 마운트 할 볼륨의 이름
          mountPath: /mount1    # 마운트 할 경로
      - name: container2
        image: kubetm/init
        volumeMounts:
        - name: empty-dir
          mountPath: /mount2
      volumes:
      - name : empty-dir        # 마운트 될 볼륨 이름
        emptyDir: {}
    ```

    ```bash
    exec로 container1 접속
    mount | grep mount1
    echo "file context" >> file.txt
    container2에서 확인
    ```

---

### hostPath

- 더 큰 단위로 파드가 묶인 한 노드에서 볼륨을 공유한다.
- 파드가 종료되도 해당 노드에 있는 데이터는 지워지지 않는다.
- 다른 파드가 종료되고 스케쥴러가 다른 노드에 배정하게 된다면 문제가 생긴다.
- 노드에 추가 될 때마다 똑같은 이름, 경로로 마운트를 걸어준다.
- 노드에있는 데이터를 파드가 파일을 읽거나 써야 할 때 사용한다.
- ▼ hostPath Type ▼
- DirectoryOrCreate : 실제 경로가 없다면 생성
- Directory : 실제 경로가 있어야됨
- FileOrCreate : 실제 경로에 파일이 없다면 생성
- File : 실제 파일이 었어야함

![hostpath](https://kubetm.github.io/img/practice/beginner/Volume%20with%20hostPath%20for%20Kubernetes.jpg)

1. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-volume-1
    spec:
      nodeSelector:
        kubernetes.io/hostname: vm-sbae-node1-ver2  # 원하는 노드에 할당
      containers:
      - name: container
        image: kubetm/init
        volumeMounts:
        - name: host-path
          mountPath: /mount1                        # 마운트 경로
      volumes:
      - name : host-path
        hostPath:
          path: /node-v                             # 노드의 볼륨 경로 - 사전에 해당 Node에 경로가 있어야한다.
          type: DirectoryOrCreate                   # 타입 설정 - 만약 이 경로가 없다면 직접 디렉토리를 생성한다.
    ```

    ```bash
    exec 컨테이너 접속 후 파일 생성
    파드 삭제 후 재생성 후 확인
    ```

---

### PVC / PV

- 파드에 영속성 있는 볼륨을 제공하기 위함.
- 외부에 사용되는 볼륨을 사용하기 위해서
- Pod ► Persistent Voulue Claim ► Persistent Volume ► 외부 Volume
- PVC, PV를 나눈 것은 사용자와 운영자를 나누기 위해서 사용된다.
- PV 생성 ► PVC 생성 ► PV 연결 ► Pod 생성 시 PVC 마운팅
- 한 번 바인딩되면 다른 클레임에서는 사용이 불가능하다.

![pvc/pv](https://kubetm.github.io/img/practice/beginner/Volume%20with%20PersistentVolume%20PersistentVolumeClaim%20for%20Kubernetes.jpg)

1. PersistentVolume

    ```yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: pv-01
    spec:                       # PV와 PVC의 설정이 맞아야한다.
      capacity:
        storage: 2G             # 볼륨 요구
      accessModes:              # 권한
      - ReadWriteOnce
      local:
        path: /node-v           # 노드에 이미 만들어져 있는 경로를 사용한다. - 없으면 에러
      nodeAffinity:
        required:
          nodeSelectorTerms:
          - matchExpressions:   # 파드가 생성 할 때마다 해당 노드에 생성 된다.
            - {key: kubernetes.io/hostname, operator: In, values: [k8s-node1]}
    ```

2. PersistentVolumeClaim

    ```yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: pvc-01
    spec:
      accessModes:
      - ReadWriteOnce               # 권한 설정
      resources:
        requests:
          storage: 2G               # 볼륨을 할당해달라고 요청
      storageClassName: ""          # 현재 만들어진 PV에 자동으로 연결된다. - 생략 시 다르게 실행된다.
    ```

3. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-volume-1
    spec:
      containers:
      - name: container
        image: kubetm/init
        volumeMounts:
        - name: pvc-pv
          mountPath: /mount1
      volumes:
      - name : pvc-pv
        persistentVolumeClaim:  # 만들어 놓은 PVC 이름을 넣는다
          claimName: pvc-01
    ```

---

## ConfigMap

- 개발환경과 상용환경을 나눠서 사용할 때 개발된 파드를 옮길 때 ssh, user, key가 바뀌어야 하는데 따로 이미지를 관리할 수 없어서 사용한다.
- 관리해야하는 일반적인 상수들 ssh, user 등을 모아서 ConfigMap 만듦
- key와 같이 보안적인 관리가 필요한 것들은 Secret을 만든다.
- 파드 생성시 오브젝트 연결이 가능해서 컨테이너의 환경변수에 데이터가 들어간다.
- 비어있는 컨테이너를 만들어서 환경에 따라서 맞게 변경해준다.
- 데이터로 상수, 파일로 넣을수 있다. | 파일로 넣을 때는 볼륨을 마운트해서 사용한다.
- key:value는 모두 String 값
- ▼ 명령어 Tip ▼

  ```bash
  # ConfigMap
  # file-c.txt 라는 파일로 cm-file라는 이름의 ConfigMap 생성
  kubectl create configmap cm-file --from-file=./file-c.txt
  # key1:value1 라는 상수로 cm-file라는 이름의 ConfigMap 생성
  kubectl create configmap cm-file --from-literal=key1=value1
  # 여러 key:value로 cm-file라는 이름의 ConfigMap 생성
  kubectl create configmap cm-file --from-literal=key1=value1 --from-literal=key2=value2

  # Secret
  # file-s.txt 라는 파일로 sec-file라는 이름의 Secret 생성
  kubectl create secret generic sec-file --from-file=./file-s.txt
  # key1:value1 라는 상수로 sec-file라는 이름의 Secret 생성
  kubectl create secret generic sec-file --from-literal=key1=value1
  ```

![configMap](https://kubetm.github.io/img/practice/beginner/ConfigMap%2C%20Secret%20with%20Literal%2C%20File%20on%20Env%2C%20Mount%20for%20Kubernetes.jpg)

### Env - Literal

![env-literal](https://kubetm.github.io/img/practice/beginner/ConfigMap%2C%20Secret%20with%20Literal%20for%20Kubernetes.jpg)

1. ConfigMap

   - key:value 형태로 구성하여 필요한 상수를 파드 생성시 컨테이너에 환경변수 세팅이 가능하다.

    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cm-dev
    data:
      SSH: 'false'  # key:value 형태로 상수를 관리
      User: dev
    ```

2. Secret

    - key:value 형태 | ex) PW:Base64
    - value는 Base64로 인코딩해서 값을 넣어야한다.

    ```yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: sec-dev
    data:
      Key: MTIzNA==     # key:value 형태로 상수를 관리
    ```

3. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-1
    spec:
      containers:
      - name: container
        image: kubetm/init
        envFrom:
        - configMapRef:
            name: cm-dev    # Configmap yaml 파일
        - secretRef:
            name: sec-dev   # secret yaml 파일
    ```

---

### Env - File

![env-file](https://kubetm.github.io/img/practice/beginner/ConfigMap%2C%20Secret%20with%20File%20for%20Kubernetes.jpg)

1. Configmap

    - 상수들을 따로 파일로 작성하여 관리한다.
    - 대시보드에서 지원해주지 않아서 cm-file을 만들꺼고 file-c.txt를 넣을 것이라는 명령어 실행

    ```bash
    echo "Content" >> file-c.txt
    kubectl create configmap cm-file --from-file=./file-c.txt
    ```

2. Secret

   - 명령어 실행 중 file-s 자체가 한 번 더 인코딩 되어 2번 인코딩 될 수 있다.

    ```bash
    echo "Content" >> file-s.txt
    kubectl create secret generic sec-file --from-file=./file-s.txt
    ```

3. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-file
    spec:
      containers:
      - name: container
        image: kubetm/init
        env:
        - name: file-c
          valueFrom:
            configMapKeyRef:
              name: cm-file
              key: file-c.txt
        - name: file-s
          valueFrom:
            secretKeyRef:
              name: sec-file
              key: file-s.txt
    ```

---

### Volume Mount - File

- 각각의 파일로 만들어 놓는 것 까지는 동일하다.
- 파드를 만들 때 컨테이너안에 경로 정의 후 파일을 마운트할 수 있다.
- 파드를 만들 때 Env(File) 방식은 한 번 주입하면 끝이라 파일을 변경해도 파드의 환경변수는 바뀌지 않는다. 재생성 되어야 바뀜
- 마운트 방식은 파일 내용을 변경하면 파드의 환경변수 내용도 변경된다.

![volume mount](https://kubetm.github.io/img/practice/beginner/ConfigMap%2C%20Secret%20with%20Mount%20for%20Kubernetes.jpg)

1. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-mount
    spec:
      containers:
      - name: container
        image: kubetm/init
        volumeMounts:
        - name: file-volume
          mountPath: /mount
      volumes:
      - name: file-volume
        configMap:
          name: cm-file
    ```

---

## Namespace

- Namespace로 따로 나누어 Resource Quota로 각 namespace마다 사용 할 수 있는 최대 자원을 제한하고 Limit Range로 들어오는 파드의 최대 크기를 제한할 수 있다.
- Resource Quota, Limit Range는 cluster에도 설정이 가능하다.
- namespace는 다른 namespace랑 분리되어 있다.
- Namespace를 지우면 파드, 서비스 등이 같이 지워진다.
- Node, PV로 공유가 가능할 수 있다.
- ▼ 명령어 ▼

    ```bash
    # nm-3의 Namespace에 있는 ResourceQuota들의 상세 조회
    kubectl describe resourcequotas --namespace=nm-3
    # nm-5의 Namespace에 있는 LimitRange들의 상세 조회
    kubectl describe limitranges --namespace=nm-5
    ```

![namespace](https://kubetm.github.io/img/practice/beginner/Namespace%2C%20ResourceQuota%2C%20LimitRange%20for%20Kubernetes.jpg)

![names](https://kubetm.github.io/img/practice/beginner/How%20to%20use%20Namespace%20for%20Kubernetes.jpg)

1. Namespace
   - 이름이 nm-1인 namespace를 만든다.

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: nm-1
    ```

2. Pod
   - namespace를 지정하여 파드를 생성한다.

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-1
      namespace: nm-1
      labels:
        app: pod
    spec:
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
    ```

3. Service
   - 서비스도 namespace를 지정하여 만들어준다.

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-1
      namespace: nm-1
    spec:
      selector:
        app: pod
      ports:
      - port: 9000
        targetPort: 8080
    ```

4. Namespace

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: nm-2
    ```

5. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-1
      namespace: nm-2
      labels:
        app: pod
    spec:
      containers:
      - name: container
        image: kubetm/init
        ports:
        - containerPort: 8080
    ```

---

### Namespace Exception

- nm-1 pod ► nm-2 pod와 같이 ip로는 접근이 가능하다.
- pod, service 둘 다 접근 가능.
- nm-2에 nodeport 생성 시 nm-1이 같은 포트를 사용하지 않도록 해야한다.
- nm-1 pod에 hostpath 생성 후 nm-2 pod에 똑같이 생성한다면 node의 볼륨을 공유한다.
- 이런 점을 방지하기 위하여 유저 별로 권한을 따로 주거나 하는 과정이 필요하다.

1. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-2
    spec:
      ports:
      - port: 9000
        targetPort: 8080
        nodePort: 30000
      type: NodePort
    ```

2. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
     name: pod-2
    spec:
      nodeSelector:
        kubernetes.io/hostname: k8s-node1
      containers:
      - name: container
        image: kubetm/init
        volumeMounts:
        - name: host-path
          mountPath: /mount1
      volumes:
      - name : host-path
        hostPath:
          path: /node-v
          type: DirectoryOrCreate
    ```

---

### ResourceQuota

- Namespace의 자원한계를 설정해준다.
- ResourceQuota를 설정한 Namespace에 Pod를 만들려고 할 때 requests, limit를 반드시 설정해줘야한다.
- ex) ResourceQuota requests = 3Gi 설정일 때 이미 한 파드가 2Gi를 차지하고 다른 파드가 2Gi를 요청한다면 해당 파드는 만들어지지 않는다.
- cpu, memory, storage 제한가능
- Pod, Service, ConfigMap, PVC 등등 숫자 제한이 가능하다. - 버전별 상이

![resourcequota](https://kubetm.github.io/img/practice/beginner/How%20to%20use%20ResourceQuota%20for%20Kubernetes.jpg)

1. Namespace
    - nm-3 생성

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: nm-3
    ```

2. ResourceQuota

    - 이름 rq-1, 메모리 권장 1Gi, 최대 1Gi로 제한을 둔 ResourceQuota를 nm-3에 맞춰서 생성한다.
    - 이미 namespace에 pod를 만든 후에 ResourceQuota를 생성해도 잘 생성된다.
    - 하지만 이 경우는 좋지않기 때문에 ResourceQuota를 생성하기 전 pod를 전부 지워주고 생성하는 것을 권장한다.
    - ▼ ResourceQuota check command ▼
    - kubectl describe resourcequotas --namespace=nm-3

    ```yaml
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: rq-1
      namespace: nm-3
    spec:
      hard:
        requests.memory: 1Gi
        limits.memory: 1Gi
    ```

    - 파드의 갯수를 제한하는 것도 가능.

    ```yaml
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: rq-2
    spec:
      hard:
        pods: 2
    ```

3. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-2
    spec:
      containers:
      - name: container
        image: kubetm/app
    ```

4. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-3
    spec:
      containers:
      - name: container
        image: kubetm/app
        resources:
          requests:
            memory: 0.5Gi
          limits:
            memory: 0.5Gi
    ```

---

### LimitRange

- 각각의 Pod마다 Namespace에 들어올 수 있는지 자원을 검사한다.
- min : 최저, max : 최대, maxLimitRequestRatio : requests와 limits 값의 비율이 해당 값을 넘지 않아야 한다.
- defautlRequest, default 설정 후 pod가 값을 설정하지 않고 들어오면 자동으로 requests, limits 값으로 설정된다.
- 각각의 Type마다 지원되는 옵션이 다르다. - 버전별로도 상이

![limitrange](https://kubetm.github.io/img/practice/beginner/How%20to%20use%20LimitRange1%20for%20Kubernetes.jpg)

1. Namespace
    - nm-5 생성

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: nm-5
    ```

2. LimitRange

    - 컨테이너에 제한을 건다.
    - 최소 0.1, 최대 0.4, 들어오는 컨테이너의 최소 최대 비율은 3, 파드 리소스 미 작성 시 최소 0.1, 최대 0.2
    - ▼ LimitRange Check Command ▼
    - kubectl describe limitranges --namespace=nm-5

    ```yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: lr-1
    spec:
      limits:
      - type: Container
        min:
          memory: 0.1Gi
        max:
          memory: 0.4Gi
        maxLimitRequestRatio:
          memory: 3
        defaultRequest:
          memory: 0.1Gi
        default:
          memory: 0.2Gi
    ```

3. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-1
    spec:
      containers:
      - name: container
        image: kubetm/app
        resources:
          requests:
            memory: 0.1Gi
          limits:
            memory: 0.5Gi
    ```

---

### LimitRange Exception

- 한 개의 namespace에 여러개의 LimitRange를 넣을 수 있다.
- 이 경우 다른 예상치 못한 상황이 발생 할 수 있기 때문에 주의가 필요하다.

1. Namespace

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: nm-6
    ```

2. LimitRange

    ```yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: lr-5
    spec:
      limits:
      - type: Container
        min:
          memory: 0.1Gi
        max:
          memory: 0.5Gi
        maxLimitRequestRatio:
          memory: 1
        defaultRequest:
          memory: 0.5Gi
        default:
          memory: 0.5Gi
    ```

3. LimitRange

    ```yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: lr-3
    spec:
      limits:
      - type: Container
        min:
          memory: 0.1Gi
        max:
          memory: 0.3Gi
        maxLimitRequestRatio:
          memory: 1
        defaultRequest:
          memory: 0.3Gi
        default:
          memory: 0.3Gi
    ```

4. Pod

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-1
    spec:
      containers:
      - name: container
        image: kubetm/app
    ```

---

## Controller

### Auto Healing

- 노드나 파드가 장애가오면 컨트롤러가 즉각적으로 인지하여 다른 노드에 새로 생성해준다.

### Auto Scaling

- 하나의 파드가 한계치에 달했을 때 컨트롤러가 인지하여 파드를 하나 더 생성하여 부하를 분산시킨다.

### Software Update

- 여러 파드들을 버전 업을 해야할 때 도와주고 문제가 생기면 백업까지 가능하다.

### Job_

- 일시적인 작업을 해야할 때 필요한 순간에만 파드를 생성하여 이행하고 그 후 파드를 삭제한다.
- 효율적인 자원 활용

![con](https://kubetm.github.io/img/practice/beginner/Controller%20with%20Replicastion%20Controller%2C%20ReplicaSet%20for%20Kubernetes.jpg)

---

## Replication

### Template, Replicas

- controller와 pod는 label과 selector로 연결되어 있다.
- replication이 label과 하나의 pod라도 일치하지 않으면 연결하지 않는다.
- template에 저장된 내용으로 종료된 pod를 재생성해준다.
- template에 버전을 업그레이드한 pod를 넣어주고 기존에 연결된 pod의 연결을 끊어주면 버전 업된 pod로 바뀐다.
- replicas의 숫자를 늘려주면 그 숫자만큼 파드가 생성된다.
- template와 같이 사용하게 된다면 해당 template에 있는 pod를 ReplicaSet의 숫자만큼 생성한다.

    ![template, replicas](https://kubetm.github.io/img/practice/beginner/Controller%20with%20Replication%2C%20ReplicaSet%20for%20Kubernetes.jpg)

1. Pod
    - label을 정해준다.

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod1
      labels:
        type: web
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
    ```

2. ReplicaSet

    - pod의 label에 맞춰서 selector해주고 template에 pod명을 맞춰 넣어준다.

    ```yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: replica1
    spec:
      replicas: 1
      selector:
        matchLabels:
          type: web
      template:
        metadata:
          name: pod1
          labels:
            type: web
        spec:
          containers:
          - name: container
            image: kubetm/app:v1
          terminationGracePeriodSeconds: 0
    ```

---

### Updating Controller

1. ReplicationController

    ```yaml
    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: replication1
    spec:
      replicas: 2
      selector:
        cascade: "false"
      template:
        metadata:
          labels:
            cascade: "false"
        spec:
          containers:
          - name: container
            image: kubetm/app:v1
    ```

2. kubectl

    ```bash
    # 파드는 남기고 컨트롤러만 지우는 명령어
    kubectl delete replicationcontrollers replication1 --cascade=false
    ```

3. ReplicaSet

    ```yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: replica2
    spec:
      replicas: 2
      selector:
        matchLabels:
          cascade: "false"
      template:
        metadata:
          labels:
            cascade: "false"
        spec:
          containers:
          - name: container
            image: kubetm/app:v1
    ```

---

### Selector

1. ReplicaSet
    - matchLabels = replication의 효과 - label이 하나라도 맞지않으면 연결하지 않음.
    - matchExpressions key, value를 좀 더 자세하게 설정가능. 값이 일치하지 않더라도 설정한 값에 맞으면 연결함.
    - key:?, operator:?, values:? 등등
    - ⭐️ ▼ operator option ▼ ⭐️
    - Exists        : 설정한 key에 맞는 pod들을 연결한다.
    - DoesNotExist  : 설정한 key에 맞지않는 pod들을 연결한다.
    - In            : 설정한 key, value에 일치하는 pod를 연결한다.
    - NotIn         : 설정한 key는 일치하고 value에 값을 제외한 pod를 연결한다.

    ![matchExpressions](https://kubetm.github.io/img/practice/beginner/Match2%20with%20Replication%2C%20ReplicaSet%20for%20Kubernetes.jpg)

    ```yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: replica1
    spec:
      replicas: 1
      selector:
        matchLabels:
          type: web
          ver: v1
        matchExpressions:
        - {key: type, operator: In, values: [web]}
        - {key: ver, operator: Exists}
      template:
        metadata:
          labels:
            type: web
            ver: v1
            location: dev
        spec:
          containers:
          - name: container
            image: kubetm/app:v1
          terminationGracePeriodSeconds: 0
    ```

---

## Deployment

- 현재 운영중인 서비스를 업데이트를 위하여 재배포를 하기위해 사용됨

![deployment](https://kubetm.github.io/img/practice/beginner/Deployment%20with%20ReCreate%2C%20RollingUpdate%20for%20Kubernetes.jpg)

### ReCreate

- 파드들을 삭제 한 후에 다시 재배포하는 방식
- 모든 자원 사용을 멈춘다.
- 서비스를 일시정지를 해야한다.
- deployment template v2로 변경 ► v1 replicaset 0으로 변경 ► 다운타임 발생 ► v2 replicaset 생성 후 서비스 연결

![recreate](https://kubetm.github.io/img/practice/beginner/Deployment%20with%20ReCreate%20for%20Kubernetes.jpg)

1. Deployment

- revisionHistoryLimit : 0이된 구버전 ReplicaSet을 몇 개나 남길지 여부.

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: deployment-1
    spec:
      selector:
        matchLabels:
          type: app
      replicas: 2
      strategy:
        type: Recreate
      revisionHistoryLimit: 1
      template:
        metadata:
          labels:
            type: app
        spec:
          containers:
          - name: container
            image: kubetm/app:v1
          terminationGracePeriodSeconds: 10
    ```

1. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-1
    spec:
      selector:
        type: app
      ports:
      - port: 8080
        protocol: TCP
        targetPort: 8080
    ```

2. Command

    ```bash
    while true; do curl ip:port/version; sleep 1; done
    # replicaset이 몇개가 있는지
    kubectl rollout history deployment deployment-1
    # 롤백
    kubectl rollout undo deployment deployment-1 --to-revision=2
    ```

---

### RollingUpdate

- 버전 업된 파드를 하나 새로 시작한다.
- 자원도 버전 업된 파드만큼 추가된다.
- 구 버전 파드를 삭제한다.
- 배포 중 추가 자원을 요구하지만 서비스를 유지할 수 있다.
- deployment template v2로 변경 ► v2 replicaset 1개 추가 ► v1 replicaset 1개 감소 ► 2,3 반복하여 서비스 버전 변경

![rollingupdate](https://kubetm.github.io/img/practice/beginner/Deployment%20with%20RollingUpdate%20for%20Kubernetes.jpg)

1. Deployment

    - RollingUpdateminReadySeconds : 파드들이 추가되고 삭제되는 과정에 대해 시간 부여

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: deployment-2
    spec:
      selector:
        matchLabels:
          type: app2
      replicas: 2
      strategy:
        type: RollingUpdate
      minReadySeconds: 10
      template:
        metadata:
          labels:
            type: app2
        spec:
          containers:
          - name: container
            image: kubetm/app:v1
          terminationGracePeriodSeconds: 0
    ```

2. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-2
    spec:
      selector:
        type: app2
      ports:
      - port: 8080
        protocol: TCP
        targetPort: 8080
    ```

3. Command

    ```bash
    while true; do curl ip:port/version; sleep 1; done
    ```

---

### Blue/Green

- controller로 pod와 service가 label로 연결되어있을 때 새로운 controller, pod, label로 만든 후 service에서 연결되는 label을 바꿔준다.
- 서비스 유지가능
- 문제시 구 버전으로 롤백가능
- 추가 자원 사용량을 요구한다.

1. ReplicaSet

    ```yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: replica1
    spec:
      replicas: 2
      selector:
        matchLabels:
          ver: v1
      template:
        metadata:
          name: pod1
          labels:
            ver: v1
        spec:
          containers:
          - name: container
            image: kubetm/app:v1
          terminationGracePeriodSeconds: 0
    ```

2. Service

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-3
    spec:
      selector:
        ver: v1
      ports:
      - port: 8080
        protocol: TCP
        targetPort: 8080
    ```

---

### Canary

- label로 연결하여 실험적인 controller, pod를 만들어서 ingress controller를 이용하여 안정적으로 유지되는지 검증한 후에 새로 배포한다.
- 안정적이라면 구 버전을 삭제한다.

## DaemonSet, Job, CronJob

- 노드의 자원상대와는 상관없이 모든 노드에 파드를 1개씩 생성한다.
- Performance : ex) prometheus  성능 수집
- Logging     : ex) fluentd     로그 수집
- Storage     : ex) GlusterFS   네트워크 파일 시스템 관리
- 직접 생성한           pod는 노드 장애 발생시 종료된다.
- replicaset으로 생성한 pod는 노드 장애 발생시 다른 노드로 재생성되고 사용되지 않으면 재시작한다.
- job으로 생성한        pod는 노드 장애 발생시 다른 노드로 재생성되고 사용되지않으면 자원을 사용하지 않는 상태가 된다.

![demonset](https://kubetm.github.io/img/practice/beginner/Controller%20with%20DatemonSet%2C%20Job%2C%20CronJob%20for%20Kubernetes.jpg)

### DaemonSet - Hostport

- selector, template로 관리
- nodeSelector를 이용하여 원하는 노드에만 파드 생성 가능

1. DaemonSet - Hostport

    - nodeSelector : 해당 라벨이 붙은 노드에만 설치
    - hostport로 들어온 트래픽은 정해놓은 container port로 들어가게 된다. | nodeport랑 비슷?
    ![damon host](https://kubetm.github.io/img/practice/beginner/HostPort%2C%20NodeSelector%20with%20DaemonSet%20for%20Kubernetes.jpg)

    ```yaml
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: daemonset-1
    spec:
      selector:
        matchLabels:
          type: app
      template:
        metadata:
          labels:
            type: app
        spec:
          containers:
          - name: container
            image: kubetm/app
            ports:
            - containerPort: 8080
              hostPort: 18080
    ```

    ```bash
    curl ip:port/hostname
    ```

---

### DaemonSet - NodeSelector

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-2
spec:
  selector:
    matchLabels:
      type: app
  template:
    metadata:
      labels:
        type: app
    spec:
      nodeSelector:
        os: centos              # 선택한 라벨에만 설치
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
```

```bash
# Label add
kubectl label nodes k8s-node1 os=centos
kubectl label nodes k8s-node2 os=ubuntu

# Label Remove
kubectl label nodes k8s-node2 os-
```

---

### Job

- selector, template으로 관리
- 특정 작업만하고 종료되는 일을 함
- selector 미 지정시 자동 생성
- completions           : 여러 개의 파드를 설정하게되면 일련의 파드들이 다 실행해야 종료된다.
- parallelism           : 한 번에 일하는 파드 갯수
- activeDeadlineSeconds : 한 job이 실행 될 수 있는 시간
- restartPolicy         : 추후 설명

![job](https://kubetm.github.io/img/practice/beginner/Parrallelism%2C%20Completions%20with%20Job%20for%20Kubernetes.jpg)

1. job1

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: job-1
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
          terminationGracePeriodSeconds: 0
    ```

2. job2

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: job-2
    spec:
      completions: 6
      parallelism: 2
      activeDeadlineSeconds: 30
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
          terminationGracePeriodSeconds: 0
    ```

---

### CronJob

- job을 주기적으로 생성하는 역할을 한다.
- 데이터 백업, 업데이트 체크, 메시지 발송 등등
- ▼ concurencyPolicy ▼
- Allow   : 이미 전에 실행 중이거나 종료하는 것과는 상관없이 정해진 조건이 될 때마다 실행한다.
- Forbid  : 이미 전에 실행 중인 파드가 종료되지 않았다면 다음 스케쥴은 스킵되고 종료되는 즉시 다른 스케쥴이 시작된다.
- Replace : 이미 전에 실행 중인 파드가 계속 실행 중이라면 첫 번째 job의 연결을 다음 스케쥴 파드에 연결해준다.
![cronjob](https://kubetm.github.io/img/practice/beginner/Allow%20with%20CronJob%20for%20Kubernetes.jpg)

1. CronJob

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cron-job
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
          terminationGracePeriodSeconds: 0
```

```bash
kubectl create job --from=cronjob/cron-job cron-job-manual-001
kubectl patch cronjobs cron-job -p '{"spec" : {"suspend" : false }}'
```

---

### Cronjob - ConcurrencyPolicy

![cronjobcon](https://kubetm.github.io/img/practice/beginner/ConcurencyPolicy%201.19%20with%20CronJob%20for%20Kubernetes.jpg)

1. Cronjob

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cron-job-2
spec:
  schedule: "20,21,22 * * * *"    # 20분 21분 22분 - 현재시간 초, 분, 시, 일, 월, 년 순서?
  concurrencyPolicy: Replace      # cronjob의 모드 설정
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 140; echo 'job end'"] # 휴면시간 140초
          terminationGracePeriodSeconds: 0
```

### CronJob - 1.19ver 이후 변경사항

- 1.19버전 이후 변경사항
- Cronjob 삭제시 Manual로 만든 Job도 같이 삭제됨
- Replace 모드 : 기존 Job은 삭제되고 (기존 Pod도 같이 삭제됨), 새 Job이(새 pod 생성) 만들어집니다.
